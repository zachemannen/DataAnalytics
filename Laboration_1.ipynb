{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jet0RhBCH6vk"
      },
      "source": [
        "# Laboration 1 - Analys av tweets från bokmässan\n",
        "\n",
        "## Attribution David Johnsson, Uppsala University"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQkkVauH6vn"
      },
      "source": [
        "Starta med att ladda in följande moduler och sätt upp visualiseringsmiljön för matplotlib\n",
        "\n",
        "1. `pandas` Ett bibliotek för att hantera data i tabellform, ett av de absolut vanligaste biblioteken för data analytics.Vi kommer gå igenom pandas mer senare i kursen.\n",
        "2. `textmining` \n",
        "Funktioner för statistisk textmining, fokuserad på bag-of-words model (som ni inte behöver sätta er in för denna kurs.f För den nyfikne eller vetgirige finns enkla förklaringar exempelvis [här](https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/) eller [här](https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/), en enkel tutorial finns också [här](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)) \n",
        "3. `wordcloud` - En visualiseringsmodul för att skapa ordmoln, vilket vi gör i denna laboration.\n",
        "4. `matplotlib` - Ett bibliotek för att skapa visualiseringar av tabelldata. \n",
        "5. `sklearn` -  Scikit-learn,ett pythonbibliotek för maskininlärningsalgoritmer, den kommer vi använda mycket i både laboration 3 och 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xJBaizZaH6vo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/margob/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /Users/margob/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/margob/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/margob/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /Users/margob/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NQASGPwvH6vp"
      },
      "outputs": [],
      "source": [
        "# Kör denna cell för att ladda in biblioteken och sätta upp vår miljö\n",
        "import pandas as pd\n",
        "import nltk as tm\n",
        "from nltk.corpus import stopwords\n",
        "import wordcloud\n",
        "import matplotlib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Sätt upp visualiseringen\n",
        "%matplotlib inline\n",
        "matplotlib.pyplot.rcParams['figure.figsize'] = [10, 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SdwBMbSYH6vq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/margob/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tm.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2NzG4XofH6vq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alla',\n",
              " 'allt',\n",
              " 'att',\n",
              " 'av',\n",
              " 'blev',\n",
              " 'bli',\n",
              " 'blir',\n",
              " 'blivit',\n",
              " 'de',\n",
              " 'dem',\n",
              " 'den',\n",
              " 'denna',\n",
              " 'deras',\n",
              " 'dess',\n",
              " 'dessa',\n",
              " 'det',\n",
              " 'detta',\n",
              " 'dig',\n",
              " 'din',\n",
              " 'dina',\n",
              " 'ditt',\n",
              " 'du',\n",
              " 'där',\n",
              " 'då',\n",
              " 'efter',\n",
              " 'ej',\n",
              " 'eller',\n",
              " 'en',\n",
              " 'er',\n",
              " 'era',\n",
              " 'ert',\n",
              " 'ett',\n",
              " 'från',\n",
              " 'för',\n",
              " 'ha',\n",
              " 'hade',\n",
              " 'han',\n",
              " 'hans',\n",
              " 'har',\n",
              " 'henne',\n",
              " 'hennes',\n",
              " 'hon',\n",
              " 'honom',\n",
              " 'hur',\n",
              " 'här',\n",
              " 'i',\n",
              " 'icke',\n",
              " 'ingen',\n",
              " 'inom',\n",
              " 'inte',\n",
              " 'jag',\n",
              " 'ju',\n",
              " 'kan',\n",
              " 'kunde',\n",
              " 'man',\n",
              " 'med',\n",
              " 'mellan',\n",
              " 'men',\n",
              " 'mig',\n",
              " 'min',\n",
              " 'mina',\n",
              " 'mitt',\n",
              " 'mot',\n",
              " 'mycket',\n",
              " 'ni',\n",
              " 'nu',\n",
              " 'när',\n",
              " 'någon',\n",
              " 'något',\n",
              " 'några',\n",
              " 'och',\n",
              " 'om',\n",
              " 'oss',\n",
              " 'på',\n",
              " 'samma',\n",
              " 'sedan',\n",
              " 'sig',\n",
              " 'sin',\n",
              " 'sina',\n",
              " 'sitta',\n",
              " 'själv',\n",
              " 'skulle',\n",
              " 'som',\n",
              " 'så',\n",
              " 'sådan',\n",
              " 'sådana',\n",
              " 'sådant',\n",
              " 'till',\n",
              " 'under',\n",
              " 'upp',\n",
              " 'ut',\n",
              " 'utan',\n",
              " 'vad',\n",
              " 'var',\n",
              " 'vara',\n",
              " 'varför',\n",
              " 'varit',\n",
              " 'varje',\n",
              " 'vars',\n",
              " 'vart',\n",
              " 'vem',\n",
              " 'vi',\n",
              " 'vid',\n",
              " 'vilka',\n",
              " 'vilkas',\n",
              " 'vilken',\n",
              " 'vilket',\n",
              " 'vår',\n",
              " 'våra',\n",
              " 'vårt',\n",
              " 'än',\n",
              " 'är',\n",
              " 'åt',\n",
              " 'över'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopWords = set(stopwords.words('swedish'))\n",
        "stopWords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNAM61sJH6vq",
        "lang": "en"
      },
      "source": [
        "## Analys av Twitterdata från bokmässan\n",
        "\n",
        "Ni har blivit inhyrda som konsulter för en bokpublicist som vill att du ska ta reda på vilka teman och böcker som har fått mest uppmärksamhet på bokmässan i Göteborg 2016. \n",
        "\n",
        "Er uppgift är att via Twitterdata undersöka vilka ämnen som fått speciellt mycket uppmärksamhet för och under bokmässan och presentera ett förslag till företaget du arbetar med vad som är lämpliga debattämnen. \n",
        "\n",
        "Fokus här är alltså på att förstå data, vilket är en viktigt del av pre-processering inför mer avacerad dataanalys. \n",
        "\n",
        "**F1.** Vad för data är distinkt för twitter och vilken typ av pre-processing tror ni kommer behövas på den typen av data? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIhr9nrQH6vr",
        "lang": "en"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "Som alltid behöver vårt data städas, i detta fall är fokus att sortera bort data som antingen inte går att analysera eller inte är intressant från den råtextdata vi fått från Twitter. Den data som givits samlades in från Twitter från maj till september 2016.\n",
        "\n",
        "Er datafil finns i mappen data i laborationsrepositoriet och heter `twitter_book_fair_data.tsv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP3aZwD0H6vs",
        "lang": "en"
      },
      "source": [
        "### Ladda data\n",
        "\n",
        "En `.tsv` fil betyder att det är en tab-separerad fil med tabelldata (jämfört med ; separerad som vi använt tidigare)\n",
        "\n",
        "**F2** Starta arbetet med att läsa in filen med read_csv() med följande parametrar:  encoding=\"utf-8\", sep=\"\\t\" och spara i en dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOesfIykH6vt",
        "lang": "en"
      },
      "source": [
        "**F3** Inspektera den dataframe som skapats med lämpliga funktioner. Ta reda på följande:\n",
        "\n",
        "Hur ser den ut?\n",
        "Antal kolumner och rader?\n",
        "Datatyper?\n",
        "\n",
        "Glöm inte bort att när du utför operationer på en datafram så sparas ingenting om du inte skapar en variabel som du lagrar dina ändringar i! (alternativt skriver över den dataframe du har genom att sätta parametern inplace = True (default är False)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA70CpnuH6vt"
      },
      "source": [
        "**F4** Finns det nullvärden i vårt dataset? Varför/varför inte?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ4proEGH6vu"
      },
      "source": [
        "**F5.** Hur många tweets i vårt dataset är nämnanden av andra användare (alltså när `@twittername` finns med i tweeten) \n",
        "\n",
        "*Hint: Det kan vara till hjälp att använda funktionen `info()`*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0aKLENaH6vu",
        "lang": "en"
      },
      "source": [
        "**F6.** En kolumn är speciellt intressant för vår **textanalys**, extrahera den från den dataframe vi lagrat all data i och skapa en variabel där du placerar denna data, döp variablen till `tweets_corpus`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IqBlz-cH6vu",
        "lang": "en"
      },
      "source": [
        "### Emojis\n",
        "\n",
        "På Twitter är det väldigt vanligt med emojis 👍 ✨ 🐫 🎉 🚀 🤘.\n",
        "\n",
        "Dessa kan innehålla mycket information som kan vara relevant för vår analys. Dock är det ofta svårt att analysera emojis med hjälp av vanliga verktug för NLP(Natural Language Processig). \n",
        "\n",
        "Vi behöver därför ta bort dessa ur vårt utvalda dataset som skapades i uppgiften ovan.\n",
        "\n",
        "Följande kod utför detta, ni behöver inte bry er om lambda just nu, men vi kommer gå igenom det lite senare i kursen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YyBIdQGBH6vv",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tweets_corpus' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m encode2ascii \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m clean_tweets \u001b[38;5;241m=\u001b[39m tweets_corpus\u001b[38;5;241m.\u001b[39mapply(encode2ascii)\n\u001b[1;32m      3\u001b[0m clean_tweets\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tweets_corpus' is not defined"
          ]
        }
      ],
      "source": [
        "encode2ascii = lambda x: x.encode('ascii', errors='ignore').decode('utf-8')\n",
        "clean_tweets = tweets_corpus.apply(encode2ascii)\n",
        "clean_tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWe2XSDGH6vv",
        "lang": "en"
      },
      "source": [
        "**F7.** Hur påverkas kvaliteten på vår analys potentiellt av att ta bort alla emojis? Förklara svaret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJoSEeRJH6vv",
        "lang": "en"
      },
      "source": [
        "### Ta bort URLs\n",
        "Det är också vanligt att man på Twitter länkar till olika webbplatser med hjälp av URL:er, när man gör textanalys på twitterdata är det vanligt att delar av dessa URL:er dyker upp som \"mest frekventa ord\" vilket påverkar vår analys negativs. Dessa behöver därför också tas bort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS5g0sqNH6vw"
      },
      "outputs": [],
      "source": [
        "clean_tweets = clean_tweets.str.replace(r'http\\S+', '')\n",
        "clean_tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XcXU2UsH6vw"
      },
      "source": [
        "**F8.** Hur kan borttagandet av URL:er pvåerkar analysen och dess kvalitet, förklara svaret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT69AsyaH6vw",
        "lang": "en"
      },
      "source": [
        "### Funktion för att hitta mest frekventa ord \n",
        "\n",
        "Ett sätt att förstå hur olika metoder för pre-processing påverkar ett dataset kan man räkna de mest förekommande orden efter varje operation som utförs. Eftersom vi kommer vilja utföra denna räkning många gånger under arbetet är de lämpligt att skapa en funktion för det som vi kan anropa flera gånger.\n",
        "\n",
        "#### Vad är en Term Document Matrix (TDM)?\n",
        "\n",
        "En TDM är en tabell där antalet unika ord räknas för varje dokument. För att göra detta på vårt Twitterdata är det lämpligt att skapa en TDM där varje tweet är en egen vektor där varje element består av de ord som finns i den tweeten. En tweet med tre unika ord blir alltså en vektor med tre element. \n",
        "\n",
        "Nedanstående kod skapar denna TDM i form av en funktion med namn `create_term_document_matrix()`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z18BDBGH6vw"
      },
      "source": [
        "**F9** Koden nedan är inte kommenterad, lägg in kommentarer som förklarar vad som sker i koden. (No hittar dokumentationen för CountVectorizer() [här](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) och en kort beskrivning med exempel [här](https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEnz4QawH6vx"
      },
      "outputs": [],
      "source": [
        "def create_term_document_matrix(corpus, min_df=1):\n",
        "    cvec = CountVectorizer(min_df=min_df, stop_words=stopWords)\n",
        "    tfmatrix = cvec.fit_transform(corpus)\n",
        "    return pd.DataFrame(data=tfmatrix.toarray(), columns=cvec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILk1ovrXH6vx"
      },
      "source": [
        "**F10** Testa vår nya funktion genom att skapa en TDM endast för de tre första raderna i `clean_tweets` som kan sorteras ut med `.head(3)` funktionen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh_fE-55H6vx"
      },
      "outputs": [],
      "source": [
        "#kod här..\n",
        "create_term_document_matrix( )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkrRfQbvH6vx"
      },
      "source": [
        "**F11.** Hur många kolumner skapades i TDM:n med bara 3 av raderna i vårt corpus?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhEHvqMwH6vx"
      },
      "source": [
        "För att hitta de mest frekvent förekommander orden i vår TDM behöver vi räkna ord. Det är också lämpligt med en visualisering över dessa vanligast förekommande ord. Även detta kommer vi behöva göra flera gånger och därför är det återigen lämpligt att definiera en funktion `plot_top_words()` som både räknar och plottar orden i ett stapeldiagram. \n",
        "\n",
        "**F12** I nedanstående cell är funktionen definierad, men koden är återigen inte kommenterad, skapa kommentarer (eller skriv i en markdowncell) som förklarar vad funktionen gör. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLDvz6oJH6vy"
      },
      "outputs": [],
      "source": [
        "def plot_top_words(tweets, num_word_instances, top_words):\n",
        "    tdm_df = create_term_document_matrix(tweets, min_df=2)\n",
        "    word_frequencies = tdm_df[[x for x in tdm_df.columns if len(x) > 1]].sum()\n",
        "    sorted_words = word_frequencies.sort_values(ascending=False)\n",
        "    top_sorted_words = sorted_words[:num_word_instances]\n",
        "    top_sorted_words[:top_words].plot.bar()\n",
        "    return top_sorted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHXVAeU4H6vy"
      },
      "source": [
        "Nu kan vi använda `plot_top_words()` funktionen för att räkna ut de mest förekommande orden i hela vårt corpus, viktigt att ha tålamod dock för det kan ta ett tag. Nedanstående kod utför beräkningen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBmexWnYH6vy"
      },
      "outputs": [],
      "source": [
        "top_words = plot_top_words(clean_tweets, 50, 30)\n",
        "top_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVzNG0aGH6vy"
      },
      "source": [
        "**F13** Hur många gånger måste ett ord finnas i corpuset för att finnas med i resultatet `top_words` så som koden är skriven ovan?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98pedQ7FH6vy"
      },
      "source": [
        "**F14.** Hur många ord plottas i stapeldiagrammet? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8qJSdH3H6vy",
        "lang": "en"
      },
      "source": [
        "### Små bokstäver\n",
        "\n",
        "Nästa steg i pre-processingen av vårt dataset (vårt corpus) är att göra om alla bokstäver till små. \n",
        "\n",
        "**F15** \n",
        "\n",
        "a.Utför ändringen att alla stora bokstäver blir små bokstäver i `clean_tweets` och spara i en ny variabel kallad `tweets_lowered`\n",
        "\n",
        "b.Varför vill man göra det för vår analys?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJh2s1G-H6vz"
      },
      "source": [
        "**F16** Räkna ut en ny variabel med de mest förekommander (frekventa) orden, döp den till `top_words_lowered`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nks3SNKsH6vz",
        "outputId": "ab43ffbd-b40c-4bfe-c279-5ac346b7f0ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Skriv klart denna kodcell för F1.16\n",
        "\n",
        "top_words_lowered = ...\n",
        "top_words_lowered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H03xRSXhH6v0"
      },
      "source": [
        "**F17.** Har något förändrats, vad? Förklara svaret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZypjpLC3H6v0",
        "lang": "en"
      },
      "source": [
        "För att underlätta att jämföra vad våra ansträngningar får för resultat kan det vara bra att enkelt kunna jämföra olika listor med top_words.\n",
        "\n",
        "**F18 a** Skapa en ny dataframe som har två kolumner, en med de 20 mest frekventa orden från`top_words` och en med de 20 mest frekventa orden från `top_word_lowered`. Döp kolumnerna till `Top tweeted clean`och  `Top tweeted lowered`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avEK_LiSH6v0"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({\n",
        "    'Top tweeted clean': top_words[0:20].index,\n",
        "    'Top tweeted lowered': top_words_lowered[0:20].index\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9mkXeekH6v0",
        "lang": "en"
      },
      "source": [
        "**F18 b** Ett annnat sätt att göra ungefär samma sak, fast lite mer automatiskt är nedanstående kod som också jämför de första 20 orden. Gör om den så att den istället för att jämföra de 20 mest frekventa orden, jämför de ord som är **minst** förekommande i de två listorna `top_words`och `top_words_lowered`.\n",
        "\n",
        "**F19** Vad returnerar nedanstående kodrad om de två listor som jämförs är identiska? Vad returneras om de inte är identiska?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1cgrM_vH6v0"
      },
      "outputs": [],
      "source": [
        "set(top_words[0:20].index) - set(top_words_lowered[0:20].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bILAEd4H6v0"
      },
      "source": [
        "### Korta ord\n",
        "\n",
        "Korta ord har ofta inte någon egentlig betydelse, alltså behöver vi inte dessa ord. Typiska sådana ord kan vara ja, jo eller nej. Vi bestämmer oss för att alla ord som är kortare än 3 bokstäver inte innehar någon betydelse i vår analys och tar därmed bort dem. \n",
        "\n",
        "**F20** Ta bort alla ord med färre bokstäver än 3(HINT: [regular expressions](https://docs.python.org/3/howto/regex.html)), lägg den nya listan med ord (som inte innehåller ord med färre bokstäver än 3) i en variabel med namn `tweets_low_no_small`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W_yn-ZeH6v1"
      },
      "outputs": [],
      "source": [
        "tweets_low_no_small = ...#din kod här"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94XWyjSkH6v1"
      },
      "outputs": [],
      "source": [
        "#Skapar ny topplista utan korta ord\n",
        "top_words_low_no_small = plot_top_words(tweets_low_no_small, 50, 30)\n",
        "top_words_low_no_small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqXEMg93H6v1"
      },
      "source": [
        "**F21.** Efter att korta ord tagits bort, hur många gånger måste ett ord förekomma i vårt corpus för att hamna i den nya listan enligt ovan? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYOtnuIAH6v1",
        "lang": "en"
      },
      "source": [
        "### Betydelselösa ord\n",
        "\n",
        "Stop words är andra ord som inte är korta men som ändå inte har betydelse, dessa kan vara lite besvärligare att identifiera och ta bort. En möjlighet är att helt enkelt skapa en lista med sådana ord och sedan använda den listan för att filtrera ut orden ur ett corpus. Vi har ju redan tagit bort alla ord med färre bokstäver än 3, så sådana behöver vi inte lägga in i listan. \n",
        "\n",
        "Nedan är ett exempel på en lista med stoppord som är betydelselösa. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pAICOjgH6v1"
      },
      "outputs": [],
      "source": [
        "my_stop_words = [\"och\", \"det\", \"att\", \"i\", \"en\", \"jag\", \"hon\", \n",
        "                \"som\", \"han\", \"paa\", \"den\", \"med\", \"var\", \"sig\", \n",
        "                \"foer\", \"saa\", \"till\", \"aer\", \"men\", \"ett\", \n",
        "                \"om\", \"hade\", \"de\", \"av\", \"icke\", \"mig\", \"du\", \n",
        "                \"henne\", \"daa\", \"sin\", \"nu\", \"har\", \"inte\", \n",
        "                \"hans\", \"honom\", \"skulle\", \"hennes\", \"daer\", \n",
        "                \"min\", \"man\", \"ej\", \"vid\", \"kunde\", \"naagot\", \n",
        "                \"fraan\", \"ut\", \"naer\", \"efter\", \"upp\", \"vi\", \n",
        "                \"dem\", \"vara\", \"vad\", \"oever\", \"aen\", \"dig\", \n",
        "                \"kan\", \"sina\", \"haer\", \"ha\", \"mot\", \"alla\", \n",
        "                \"under\", \"naagon\", \"eller\", \"allt\", \"mycket\", \n",
        "                \"sedan\", \"ju\", \"denna\", \"sjaelv\", \"detta\", \n",
        "                \"aat\", \"utan\", \"varit\", \"hur\", \"ingen\", \"mitt\", \n",
        "                \"ni\", \"bli\", \"blev\", \"oss\", \"din\", \"dessa\", \n",
        "                \"naagra\", \"deras\", \"blir\", \"mina\", \"samma\", \n",
        "                \"vilken\", \"er\", \"saadan\", \"vaar\", \"blivit\", \n",
        "                \"dess\", \"inom\", \"mellan\", \"saadant\", \"varfoer\", \n",
        "                \"varje\", \"vilka\", \"ditt\", \"vem\", \"vilket\", \n",
        "                \"sitta\", \"saadana\", \"vart\", \"dina\", \"vars\", \n",
        "                \"vaart\", \"vaara\", \"ert\", \"era\", \"vilka\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDz3hdJXH6v2",
        "lang": "en"
      },
      "source": [
        "När vi skapat vår lista är det dags att skapa en funktion som tar bort dessa från ett dokument. Denna funktion är kodad i cellen nedan. (Igen strunta i lambda för tillfället.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNY7UKiwH6v2"
      },
      "outputs": [],
      "source": [
        "remove_stopwords = lambda x: ' '.join(y for y in x.split() if y not in my_stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtl8iWqcH6v2"
      },
      "source": [
        "Funktionen ovan tar alltså bort stoppord från ett dokument (alltså en tweet), för att ta bort stoppord från hela vårt corpus kan funktionen `.apply()`användas. \n",
        "\n",
        "**F22.** Skriv den kod som tar bort alla stoppord från `tweets_low_no_small` och skapar en ny variabel `tweets_low_no_small_stopwords` för corpuset utan stoppord."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2tKbp4CH6v2"
      },
      "outputs": [],
      "source": [
        "tweets_low_no_small_stopwords = ...#din kod här"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ek7W9THH6v2"
      },
      "outputs": [],
      "source": [
        "top_words_low_no_small_stopwords = plot_top_words(tweets_low_no_small_stopwords, 50, 30)\n",
        "top_words_low_no_small_stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3UqdYh_H6v2"
      },
      "source": [
        "**F23.** Efter att stopporden tagits bort, hur många gånger måste ett ord förekomma i vårt corpus för att hamna i den nya listan enligt ovan? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR-bM_xqH6v2",
        "lang": "en"
      },
      "source": [
        "**F24.** Vad är skillnaderna mellan de frekvent förekommande orden i jämförelse med våra tidigare listor? Skriv den kod som jämför dessa tre listor `top_words_lowered`, `top_words_low_no_small` and `top_words_low_no_small_stopwords`, titta på de första 20 orden i listorna.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwI63HRPH6v3",
        "lang": "en"
      },
      "source": [
        "### Visualisering och rekommendation\n",
        "\n",
        "Dags att visualisera vårt resultat och övertyga vår klient om att vi hittat de bästa debattämnena för dem! Här gör vi det genom att skapa ett word cloud där de mest frekventa orden syns bäst. \n",
        "\n",
        "Nedanstående kod skapar ett ordmoln för `top_words_low_no_small_stopwords`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIDAtPbCH6v3"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "wordcloud = WordCloud(max_font_size=40)\n",
        "wordcloud.fit_words(top_words_no_small_stopwords.to_dict())\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ixFS3QeH6v3"
      },
      "source": [
        "**F25** Ändra i tidigare kod hur många gånger ett ord minst måste finnas för att det ska inkluderas i ordmolnet. Vad förändras?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkk5kNvGH6v3",
        "lang": "en"
      },
      "source": [
        "**F26** När du tittar på ordmolnet, är det fler ord som borde vara stoppord? Ange några stycken och förklara varför de bör tas bort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_r02ky5H6v3",
        "lang": "en"
      },
      "source": [
        "**F27.** Vilket tema rekommenderar ni att publicisten ska ha som debattämne? Förklara svaret. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bux0fdKH6v3",
        "lang": "en"
      },
      "source": [
        "**F28.** Ni har nu arbetat med textdata, hur är det annorlunda när det gäller pre-processing jämfört med annan typ av data som är av mer numerisk eller kategorisk karaktär?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe8QUGnDH6v3"
      },
      "source": [
        "---\n",
        "*När ni besvarat samtliga frågor och all er kod fungerar i enlighet med instruktioner, glöm då inte commita er lösning till GitHub och hör sedan av er för muntlig redovisning.\n",
        "\n",
        "Lycka till!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3uAIDJZH6v4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nbTranslate": {
      "displayLangs": [],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "sv",
      "targetLang": "en",
      "useGoogleTranslate": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
